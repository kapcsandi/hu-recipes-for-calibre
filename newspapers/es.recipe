# -*- coding: utf-8 -*-
__license__   = 'GPL v3'
__copyright__ = '2010, Zsolt Botykai <zsoltika@gmail.com>'
'''
A recipe for Calibre to fetch http://www.es.hu , and generate an article list 
to fetch, then get rid of the unnecessary scrap at the site (e.g. facebook 
buttons, ads...)
'''

# The recipe modifies the case of titles and searches via regexs
import string, re
from string import capwords

from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import Tag, NavigableString

class EletEsIrodalom(BasicNewsRecipe):

    title                   = u"Élet és Irodalom"
    __author__              = 'Zsolt Botykai'
    description             = u"Élet és Irodalom"
    INDEX                   = 'http://www.es.hu/'
    language                = 'hu'
    remove_javascript       = True
    remove_empty_feeds      = True
    no_stylesheets          = True
    #needs_subscription      = True
    auto_cleanup            = True
    #auto_cleanup_keep       = True
    publication_type        = 'magazine'

    # without the background color setup, the conversion to pdf produced 
    # black pages with white text 
    extra_css = '''
                    body { background-color: white; color: black; }
                    p { text-align: justify; margin_top: 0px; }
                '''

    masthead_url='http://www.es.hu/images/logo.jpg'

    def postprocess_html(self, soup, first):
        html_title=soup.find('title').string
        new_html_title=html_title.replace(u" | ÉLET ÉS IRODALOM","")
        new_title_tag=Tag(soup, 'title')
        new_title_tag.insert(0,new_html_title)
        soup.find('title').replaceWith(new_title_tag)
        h2_title=soup.find('h2').string
        new_h2_title=h2_title.replace(u" | ÉLET ÉS IRODALOM","")
        new_h2=Tag(soup, 'h2')
        new_h2.insert(0,new_h2_title)
        soup.find('h2').replaceWith(new_h2)
        for para in soup.findAll('p'):
            para['height']=1


        return soup


    def get_browser(self):
        br = BasicNewsRecipe.get_browser()
        br.open('http://www.es.hu/')
        br.select_form(name='userfrmlogin')
        br['cusername']   = 'zsoltika@gmail.com'
        br['cpassword'] = 'zsoltika@gmail.com'
        br.submit()
        return br

    def get_cover_url(self):
        return 'http://www.es.hu/images/logo.jpg'

    def parse_index(self):
        articles = []

        soup = self.index_to_soup(self.INDEX)
        section_title = soup.find('div', attrs={'class':'fpdocument'})
        if section_title is not None:
            section_name = self.tag_to_string(section_title).strip().split()[-2:]
            if section_name:
                self.timefmt = ' [%s]'%(' '.join(section_name))

        cover = soup.find('img', src=True, attrs={'class':'cover'})
        if cover is not None:
            self.cover_url = cover['src']

        feeds = []
        for section in soup.findAll('div', attrs={'class':'fpdocument'}):
            section_title = section.find('a', attrs={'class':'rovat'})
            section_title = string.capwords(self.tag_to_string(section_title))
            if section_title == 'Szabadpolc':
                continue

            articles = []
            self.log('Found section: ', section_title)
            article_found = section.find('li')
            if article_found:
                for article in section.findAll('li'):
                    article_title = self.tag_to_string(article).replace(": "," - ")
                    article_title = capwords(article_title)
                    a = article.find('a', href=True)
                    url = a['href']
                    if url.startswith('/'):
                        url = 'http://www.es.hu'+url

                    p = article.find('p', attrs={'align':'left'})
                    desc = None
                    logurl='#'+url+'#'
                    self.log('1. \tFound article: ', article_title, 'at', logurl)

                    if p is not None:
                        desc = self.tag_to_string(p)
                        self.log('\t\t', desc)

                    articles.append({'title':article_title, 'url':url, 'description':desc,
                        'date':''})
            else:
                    article = section.find('a', attrs={'class':'title'})
                    if article:
                            article_title = self.tag_to_string(article).replace(": "," - ")
                            article_title = capwords(article_title)
                            article_author = section.find('div', attrs={'class':'author'})

                            if article_author:
                                    author_name = capwords(self.tag_to_string(article_author))

                                    if author_name != '':
                                            article_title = author_name + ' - ' + article_title

                            a = section.find('a', href=True, attrs={'class':'title'})
                            url = a['href']

                            if url.startswith('/'):
                                url = 'http://www.es.hu'+url

                            logurl='#'+url+'#'
                            self.log('2. \tFound article:', article_title , 'at', logurl)

                            articles.append({'title':article_title, 'url':url, 'description':'',
                                'date':''})

            feeds.append((section_title, articles))

        return feeds


    
# Release: v20110325
