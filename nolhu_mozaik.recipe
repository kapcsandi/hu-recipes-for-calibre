import string, re
from calibre.web.feeds.recipes import BasicNewsRecipe
from calibre.constants import config_dir, CONFIG_DIR_MODE
import os, os.path, urllib
from hashlib import md5

class NolHuRovatokmozaik(BasicNewsRecipe):
    __author__              = 'Zsolt Botykai'
    title                   = u'NOL.hu rovatok'
    description             = u'A Népszabadság főbb rovatai...'
    oldest_article          = 10000
    max_articles_per_feed   = 10000
    language                = 'hu'
    remove_javascript       = True
    remove_empty_feeds      = True
    no_stylesheets          = True
    feeds                   = [(u'Mozaik',   u'http://nol.hu/feed/mozaik.rss'),
                              ]
    remove_javascript       = True
    use_embedded_content    = False

    extra_css = '''
                    body { background-color: white; color: black }
                '''

    masthead_url='http://nol.hu/_design/image/logo_nol_live.jpg'

    preprocess_regexps      = [
        (re.compile(r'<h4>', re.IGNORECASE),
         lambda match: '<h4 style="font-size: 80%;">'),
    ]
    keep_only_tags          = [
                       dict(name='td', attrs={'class':['content']}) ,
    ]
    remove_tags = [
                       dict(name='div', attrs={'class':['ad-container-outer',\
                                                        'tags noborder',\
                                                        'video-container',\
                                                        'related-container',\
                                                        'h']}) ,
                       dict(name='div', attrs={'style':['width:17px; height:17px; background-color:#8D0648; margin-bottom:25px; float:right;']}) ,
                       dict(name='td', attrs={'class':['foot']}) ,
                       dict(name='tfoot', ) ,
    ]

    def get_cover_url(self):
        return 'http://nol.hu/_design/image/logo_nol_live.jpg'

    def convert_adurls(self,url):
        if url.find('feedsportal.com'):
            url=url.replace('0L','').\
                    replace('0B','.').\
                    replace('0C','/').\
                    replace('0E','-').\
                    replace('0I','_').\
                    replace('0A','0')
            url=url[url.rfind('nol.hu'):]

            if url.rfind('.htm'):
                url=url[:url.rfind('/')]

        return url

    def preprocess_html(self, soup):
        for h4 in soup.findAll('h4',):
            for komlink in h4.findAll('a'):
                if komlink['href'] == '#kommentek' :
                   komlink.extract()

        for alink in soup.findAll('a'):
            if alink.string is not None:
               astr = alink.string
               alink.replaceWith(astr)

        return soup

    def postprocess_html(self, soup, first):
        for t in soup.findAll(['table','tr','td','tfoot','thead','tbody']):
            t.name = 'div'

        return soup


    def parse_feeds(self):
        feeds = BasicNewsRecipe.parse_feeds(self)
        for feed in feeds:
            for article in feed.articles[:]:
                article.url='http://'+self.convert_adurls(article.url)

        return feeds


    # As seen here: http://www.mobileread.com/forums/showpost.php?p=1295505&postcount=10
    #def parse_feeds(self):
    #    recipe_dir = os.path.join(config_dir,'recipes')
    #    hash_dir = os.path.join(recipe_dir,'recipe_storage')
    #    feed_dir = os.path.join(hash_dir,self.title.encode('utf-8').replace('/',':'))
    #    if not os.path.isdir(feed_dir):
    #        os.makedirs(feed_dir,mode=CONFIG_DIR_MODE)

    #    feeds = BasicNewsRecipe.parse_feeds(self)

    #    for feed in feeds:
    #        feed_hash = urllib.quote(feed.title.encode('utf-8'),safe='')
    #        feed_fn = os.path.join(feed_dir,feed_hash)

    #        past_items = set()
    #        if os.path.exists(feed_fn):
    #           with file(feed_fn) as f:
    #               for h in f:
    #                   past_items.add(h.strip())
    #
    #        cur_items = set()
    #        for article in feed.articles[:]:
    #            item_hash = md5()
    #            if article.content: item_hash.update(article.content.encode('utf-8'))
    #            if article.summary: item_hash.update(article.summary.encode('utf-8'))
    #            item_hash = item_hash.hexdigest()
    #            if article.url:
    #                item_hash = article.url + ':' + item_hash
    #            cur_items.add(item_hash)
    #            if item_hash in past_items:
    #                feed.articles.remove(article)
    #        with file(feed_fn,'w') as f:
    #            for h in cur_items:
    #                f.write(h+'\n')

    #    remove = [f for f in feeds if len(f) == 0 and
    #            self.remove_empty_feeds]
    #    for f in remove:
    #        feeds.remove(f)

    #    return feeds

# Release: v20110325
